{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books in pdf folder:\n",
      "The_Art_of_Seduction.pdf\n",
      "like_switch.pdf\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "test = True\n",
    "\n",
    "#print out books in pdf folder\n",
    "print(\"Books in pdf folder:\")\n",
    "for file in os.listdir(\"pdfs\"):\n",
    "    print(file)\n",
    "\n",
    "\n",
    "\n",
    "file_name = input(\"Enter the name of the file you want to extract text from: \")\n",
    "directory_name = input(\"Enter the name of the book to make into file directory: \")\n",
    "\n",
    "\n",
    "#make file directory if it doesn't exist\n",
    "if not os.path.exists(directory_name):\n",
    "    os.makedirs(directory_name)\n",
    "\n",
    "# if bookname does exist then create a new directory with a number appended to the end\n",
    "else:\n",
    "    num = 1\n",
    "    while os.path.exists(f\"{directory_name}_{num}\"):\n",
    "        num += 1\n",
    "    directory_name = f\"{directory_name}_{num}\"\n",
    "    os.makedirs(directory_name)\n",
    "\n",
    "#make page_chunks directory if it doesn't exist\n",
    "if not os.path.exists(f\"{directory_name}/page_chunks\"):\n",
    "    os.makedirs(f\"{directory_name}/page_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reader = PdfReader(\"pdfs/\" + file_name)\n",
    "text_chunks = []\n",
    "\n",
    "first_run = True\n",
    "\n",
    "for page in reader.pages:\n",
    "    \n",
    "\n",
    "    if first_run:\n",
    "        first_run = False\n",
    "\n",
    "        first_page = page.extract_text() + \"\\n\"\n",
    "\n",
    "        second_page = page.extract_text() + \"\\n\"\n",
    "\n",
    "        text_chunks.append(first_page + second_page)\n",
    "\n",
    "    else:\n",
    "        first_page = second_page\n",
    "        second_page = page.extract_text() + \"\\n\"\n",
    "\n",
    "        text_chunks.append(first_page + second_page)\n",
    "\n",
    "\n",
    "for index, file_chunk in enumerate(text_chunks):\n",
    "    with open(f\"{directory_name}/page_chunks/text_chunks_{index}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "#print(\"Books in pdf folder:\")\n",
    "#for file in os.listdir(\"pdfs\"):\n",
    "#    print(file)\n",
    "#\n",
    "#print(\"-----------------------------------\")\n",
    "\n",
    "print(\"Folders in current directory\")\n",
    "for file in os.listdir():\n",
    "    if os.path.isdir(file) and file != \"pdfs\":\n",
    "        print(file)\n",
    "\n",
    "#create a pandas dataframe of all the files to store flashcards in\n",
    "flashcards_df = pd.DataFrame(columns=[\"Term\", \"Definition\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get the list of all files in the directory page_chunks\n",
    "files = os.listdir(f\"{directory_name}/page_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_sequence = \"\\nAI:\"\n",
    "restart_sequence = \"\\nHuman: \"\n",
    "\n",
    "if not os.path.exists(f\"{directory_name}/flash_chunks\"):\n",
    "    os.makedirs(f\"{directory_name}/flash_chunks\")\n",
    "\n",
    "if not os.path.exists(f\"{directory_name}/file_exceptions\"):\n",
    "    os.makedirs(f\"{directory_name}/file_exceptions\")  \n",
    "\n",
    "if not test:\n",
    "\n",
    "    for index, file in tqdm(enumerate(files[2:])):\n",
    "        \n",
    "        try:\n",
    "            file_content = open(f\"{directory_name}/page_chunks/{file}\", \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages= [{\"role\": \"system\", \"content\": \"You produce flashcards from a two-page section from a book. You produce highly detailed flash cards with a term name and a definition in the format: Term: <Card Name> \\n Definition: <Card Definition> \\n ... Term: <Card Name> \\n Defnition: <Card Definition>\"}, {\"role\": \"user\", \"content\": \"please produce some flashcards from the provided content: \\n\" + file_content}],\n",
    "            )\n",
    "\n",
    "            \n",
    "            with open(f\"{directory_name}/flash_chunks/flash_chunks_{index}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            # write file exepction to a file in file_exceptions folder\n",
    "            with open(f\"{directory_name}/file_exceptions/{file}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(str(e))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "card_deck = input(\"Enter the name of the card deck: \")\n",
    "\n",
    "\n",
    "card_files = os.listdir(f\"{directory_name}/flash_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a csv file in flash_deck folder\n",
    "df = pd.DataFrame(columns=[\"card_name\", \"card_definition\"])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for card_file in tqdm(card_files):\n",
    "    with open(f\"{directory_name}/flash_chunks/{card_file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        try:\n",
    "            cards = f.read().split(\"\\n\\n\")\n",
    "            \n",
    "            for card in cards:\n",
    "                card = card.strip()\n",
    "                card_name, card_definition = card.split(\"\\n\")\n",
    "\n",
    "\n",
    "                # if card name matches the regex of Term: <Card Name> and if card definition matches the regex of Definition: <Card Definition>\n",
    "                if card_name.startswith(\"Term: \") and card_definition.startswith(\"Definition: \"):\n",
    "\n",
    "                    card_name = card_name.replace(\"Term: \", \"\")\n",
    "                    card_definition = card_definition.replace(\"Definition: \", \"\")\n",
    "\n",
    "\n",
    "                    card_name = card_name.replace('\"', \"\")\n",
    "                    card_name = card_name.replace(\"”\", \"\")\n",
    "                    card_name = card_name.replace(\"“\", \"\")\n",
    "                    card_name = card_name.strip()\n",
    "                    card_name = card_name.lower()\n",
    "\n",
    "                    #card_definition = card_definition.replace('\"', \"'\")\n",
    "                    #card_definition.replace(\"”\", \" \")\n",
    "                    #card_definition.replace(\"“\", \" \")\n",
    "\n",
    "                    #card_definition = '\"' + card_definition + '\"'\n",
    "\n",
    "                    \n",
    "\n",
    "                    # add data to csv file\n",
    "                    card_row = pd.DataFrame({\"card_name\": [card_name], \"card_definition\": [card_definition]})\n",
    "                    df = pd.concat([df, card_row], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "\n",
    "df = df.sort_values(by=[\"card_name\"])\n",
    "\n",
    "\n",
    "df.to_csv(f\"{directory_name}/flash_decks/{card_deck}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "df = pandas.read_csv(f\"{directory_name}/flash_decks/{card_deck}.csv\")\n",
    "\n",
    "# group by card name and aggregate card definitions\n",
    "df = df.groupby(\"card_name\").agg({\"card_definition\": lambda x: \"\\n\\n\".join(x)})\n",
    "\n",
    "df.to_csv(f\"{directory_name}/flash_decks/{card_deck}_aggr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(f\"{directory_name}/flash_decks/{card_deck}_aggr.csv\")\n",
    "\n",
    "\n",
    "\n",
    "def find_and_remove_similar_strings(query_string, dataframe, column_name, similarity_threshold):\n",
    "  \"\"\"\n",
    "  Finds similar strings to the given query string in the given pandas dataframe and removes them from the dataframe.\n",
    "\n",
    "  Args:\n",
    "    query_string: The string to search for.\n",
    "    dataframe: The pandas dataframe to search in.\n",
    "    column_name: The name of the column to search in.\n",
    "    similarity_threshold: The minimum similarity score for a string to be considered similar.\n",
    "\n",
    "  Returns:\n",
    "    The updated pandas dataframe.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a text splitter.\n",
    "  text_splitter = CharacterTextSplitter()\n",
    "\n",
    "  # Create a document loader.\n",
    "  document_loader = CSVLoader(dataframe[column_name])\n",
    "\n",
    "  # Create an embedding model.\n",
    "  embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "  # Create a vector store.\n",
    "  vector_store = FAISS()\n",
    "\n",
    "  # Build the vector store.\n",
    "  vector_store.build(embedding_model, document_loader, text_splitter)\n",
    "\n",
    "  # Get the query embedding.\n",
    "  query_embedding = embedding_model.encode(query_string, text_splitter)\n",
    "\n",
    "  # Search for similar vectors.\n",
    "  similar_vectors = vector_store.search(query_embedding, 10)\n",
    "\n",
    "  # Get the similar strings and scores.\n",
    "  similar_string_score_pairs = [(document_loader.get_document(vector_id), vector_store.get_score(query_embedding, vector_id)) for vector_id in similar_vectors]\n",
    "\n",
    "  # Filter out similar strings with scores below the threshold.\n",
    "  similar_string_score_pairs = [pair for pair in similar_string_score_pairs if pair[1] >= similarity_threshold]\n",
    "\n",
    "  # Get the similar strings.\n",
    "  similar_strings = [pair[0] for pair in similar_string_score_pairs]\n",
    "\n",
    "  # Get the indices of the similar strings.\n",
    "  similar_string_indices = [dataframe[column_name] == similar_string for similar_string in similar_strings]\n",
    "\n",
    "  # Remove the similar strings from the dataframe.\n",
    "  dataframe = dataframe[~similar_string_indices]\n",
    "\n",
    "  # Return the updated dataframe.\n",
    "  return dataframe, similar_string_indices\n",
    "\n",
    "\n",
    "\n",
    "column_name = 'card_definition'\n",
    "similarity_threshold = 0.9\n",
    "\n",
    "\n",
    "reduced_df = df.copy()\n",
    "\n",
    "\n",
    "deleted_index = []\n",
    "# Iterate over the rows of the dataframe.\n",
    "for index, row in df.iterrows():\n",
    "  if index not in deleted_index:\n",
    "    reduced_df, similar_string_indices = find_and_remove_similar_strings(row[column_name], reduced_df, column_name, similarity_threshold)\n",
    "    deleted_index += similar_string_indices\n",
    "\n",
    "\n",
    "\n",
    "# save reduced_df to a csv file\n",
    "reduced_df.to_csv(f\"{directory_name}/flash_decks/{card_deck}_reduced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a flask app that serves the flashcards with two buttons: to approve or disapprove\n",
    "import random\n",
    "import genanki\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv(f\"{directory_name}/flash_decks/{card_deck}_reduced.csv\")\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "names = df[\"card_name\"].tolist()\n",
    "definitions = df[\"card_definition\"].tolist()\n",
    "\n",
    "model_id = random.randrange(1 << 30, 1 << 31)\n",
    "\n",
    "print(model_id)\n",
    "\n",
    "my_model = genanki.Model(model_id ,\n",
    "  'Knowledge',\n",
    "  fields=[\n",
    "    {'name': 'Question'},\n",
    "    {'name': 'Answer'},\n",
    "  ],\n",
    "  templates=[\n",
    "    {\n",
    "      'name': 'Card type 1',\n",
    "      'qfmt': '{{Question}}',\n",
    "      'afmt': '{{FrontSide}}<hr id=\"answer\">{{Answer}}',\n",
    "    },\n",
    "  ])\n",
    "\n",
    "\n",
    "notes = []\n",
    "\n",
    "for term, definition in zip(names, definitions):\n",
    "    question = f\"{term}\"\n",
    "    ankiAnswer = definition\n",
    "    \n",
    "    my_note = genanki.Note(\n",
    "      model=my_model,\n",
    "      fields=[f'{question}', f'{ankiAnswer}'])\n",
    "    \n",
    "    notes.append(my_note)\n",
    "    \n",
    "\n",
    "\n",
    "deck_id = random.randrange(1 << 30, 1 << 31)\n",
    "\n",
    "my_deck = genanki.Deck(deck_id, f\"{card_deck}\")\n",
    "\n",
    "for note in notes:\n",
    "    my_deck.add_note(note)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "anki_folder = \"Anki_Decks/\"\n",
    "\n",
    "anki_questions_file = anki_folder + f\"{card_deck}.apkg\"\n",
    "\n",
    "with open(anki_questions_file, \"w+\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "\n",
    "genanki.Package(my_deck).write_to_file(anki_questions_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
