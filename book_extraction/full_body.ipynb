{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#print out books in pdf folder\n",
    "print(\"Books in pdf folder:\")\n",
    "for file in os.listdir(\"pdfs\"):\n",
    "    print(file)\n",
    "\n",
    "\n",
    "\n",
    "file_name = input(\"Enter the name of the file you want to extract text from: \")\n",
    "book_name = input(\"Enter the name of the book to make into file directory: \")\n",
    "\n",
    "\n",
    "#make file directory if it doesn't exist\n",
    "if not os.path.exists(book_name):\n",
    "    os.makedirs(book_name)\n",
    "\n",
    "#make page_chunks directory if it doesn't exist\n",
    "if not os.path.exists(f\"{book_name}/page_chunks\"):\n",
    "    os.makedirs(f\"{book_name}/page_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "reader = PdfReader(\"pdfs/\" + file_name)\n",
    "text_chunks = []\n",
    "\n",
    "first_run = True\n",
    "\n",
    "for page in reader.pages:\n",
    "    \n",
    "\n",
    "    if first_run:\n",
    "        first_run = False\n",
    "\n",
    "        first_page = page.extract_text() + \"\\n\"\n",
    "\n",
    "        second_page = page.extract_text() + \"\\n\"\n",
    "\n",
    "        text_chunks.append(first_page + second_page)\n",
    "\n",
    "    else:\n",
    "        first_page = second_page\n",
    "        second_page = page.extract_text() + \"\\n\"\n",
    "\n",
    "        text_chunks.append(first_page + second_page)\n",
    "\n",
    "\n",
    "for index, file_chunk in enumerate(text_chunks):\n",
    "    with open(f\"{book_name}/page_chunks/text_chunks_{index}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "#print(\"Books in pdf folder:\")\n",
    "#for file in os.listdir(\"pdfs\"):\n",
    "#    print(file)\n",
    "#\n",
    "#print(\"-----------------------------------\")\n",
    "\n",
    "print(\"Folders in current directory\")\n",
    "for file in os.listdir():\n",
    "    if os.path.isdir(file) and file != \"pdfs\":\n",
    "        print(file)\n",
    "\n",
    "\n",
    "\n",
    "folder_name = input(\"Enter the name of the folder you want to extract text from: \")\n",
    "\n",
    "\n",
    "\n",
    "#get the list of all files in the directory page_chunks\n",
    "files = os.listdir(f\"{folder_name}/page_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_sequence = \"\\nAI:\"\n",
    "restart_sequence = \"\\nHuman: \"\n",
    "\n",
    "if not os.path.exists(f\"{folder_name}/flash_chunks\"):\n",
    "    os.makedirs(f\"{folder_name}/flash_chunks\")\n",
    "\n",
    "if not os.path.exists(f\"{folder_name}/file_exceptions\"):\n",
    "    os.makedirs(f\"{folder_name}/file_exceptions\")  \n",
    "\n",
    "\n",
    "for index, file in tqdm(enumerate(files[2:])):\n",
    "    \n",
    "    try:\n",
    "        file_content = open(f\"{folder_name}/page_chunks/{file}\", \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages= [{\"role\": \"system\", \"content\": \"You produce flashcards from a two-page section from a book. You produce highly detailed flash cards with a term name and a definition in the format: Term: <Card Name> \\n Definition: <Card Definition> \\n ... Term: <Card Name> \\n Defnition: <Card Definition>\"}, {\"role\": \"user\", \"content\": \"please produce some flashcards from the provided content: \\n\" + file_content}],\n",
    "        )\n",
    "\n",
    "        with open(f\"{folder_name}/flash_chunks/flash_chunks_{index}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # write file exepction to a file in file_exceptions folder\n",
    "        with open(f\"{folder_name}/file_exceptions/{file}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(e))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "card_deck = input(\"Enter the name of the card deck: \")\n",
    "card_deck = \"like_switch\"\n",
    "\n",
    "card_files = os.listdir(f\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/flash_chunks/{card_deck}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a csv file in flash_deck folder\n",
    "df = pd.DataFrame(columns=[\"card_name\", \"card_definition\"])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for card_file in tqdm(card_files):\n",
    "    with open(f\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/flash_chunks/{card_deck}/{card_file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        try:\n",
    "            cards = f.read().split(\"\\n\\n\")\n",
    "            \n",
    "            for card in cards:\n",
    "                card = card.strip()\n",
    "                card_name, card_definition = card.split(\"\\n\")\n",
    "\n",
    "\n",
    "                # if card name matches the regex of Term: <Card Name> and if card definition matches the regex of Definition: <Card Definition>\n",
    "                if card_name.startswith(\"Term: \") and card_definition.startswith(\"Definition: \"):\n",
    "\n",
    "                    card_name = card_name.replace(\"Term: \", \"\")\n",
    "                    card_definition = card_definition.replace(\"Definition: \", \"\")\n",
    "\n",
    "\n",
    "                    card_name = card_name.replace('\"', \"\")\n",
    "                    card_name = card_name.replace(\"”\", \"\")\n",
    "                    card_name = card_name.replace(\"“\", \"\")\n",
    "                    card_name = card_name.strip()\n",
    "                    card_name = card_name.lower()\n",
    "\n",
    "                    #card_definition = card_definition.replace('\"', \"'\")\n",
    "                    #card_definition.replace(\"”\", \" \")\n",
    "                    #card_definition.replace(\"“\", \" \")\n",
    "\n",
    "                    #card_definition = '\"' + card_definition + '\"'\n",
    "\n",
    "                    \n",
    "\n",
    "                    # add data to csv file\n",
    "                    card_row = pd.DataFrame({\"card_name\": [card_name], \"card_definition\": [card_definition]})\n",
    "                    df = pd.concat([df, card_row], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "\n",
    "df = df.sort_values(by=[\"card_name\"])\n",
    "\n",
    "\n",
    "df.to_csv(f\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/flash_decks/{card_deck}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "\n",
    "# list folders in directory C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/\n",
    "\n",
    "folders = os.listdir(\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/\")\n",
    "\n",
    "#print out folder names\n",
    "\n",
    "print(\"Folder Names:\")\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "\n",
    "\n",
    "df = pandas.read_csv(f\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/{name}/flash_decks/{name}.csv\")\n",
    "\n",
    "# group by card name and aggregate card definitions\n",
    "df = df.groupby(\"card_name\").agg({\"card_definition\": lambda x: \"\\n\\n\".join(x)})\n",
    "\n",
    "df.to_csv(f\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/{name}/flash_decks/{name}_aggr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/flash_decks/like_switch_aggr.csv\")\n",
    "\n",
    "#for value in data frame column card_definition has \\n filter into a new data frame\n",
    "\n",
    "\n",
    "df = df[df[\"card_definition\"].str.contains(\"\\n\")]\n",
    "\n",
    "\n",
    "reduced_df = pd.DataFrame(columns=[\"card_name\", \"card_definition\"])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    #try:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages= [{\"role\": \"system\", \"content\": \n",
    "                    \"You take in information and condense it into one high quality flashcard. You produce highly a detailed flashcard with a term name and a definition in the format: Term: <Card Name> \\n\\n Definition: <Card Definition> \"}, \n",
    "                    {\"role\": \"user\", \"content\": f\"Please produce a flashcard on {row['card_name']} from the following content: \\n {row['card_definition']}\"}],\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    print(text)\n",
    "    term, definition = text.split(\"\\n\\n\")\n",
    "\n",
    "    term = term.replace(\"Term: \", \"\")\n",
    "    definition = definition.replace(\"Definition: \", \"\")\n",
    "\n",
    "    reduced_df = pd.concat([reduced_df, pd.DataFrame({\"card_name\": [term], \"card_definition\": [definition]})], ignore_index=True)\n",
    "\n",
    "    reduced_df[\"card_definition\"] = reduced_df[\"card_definition\"].apply(lambda x: x.strip())\n",
    "\n",
    "# save reduced_df to a csv file\n",
    "reduced_df.to_csv(\"C:/Users/malco/OneDrive/Documents/GitHub/Auto-GPT/GPT-Tools/book_extraction/flash_decks/like_switch_reduced.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
